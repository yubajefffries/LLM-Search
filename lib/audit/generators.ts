import type { PageData, CrawlResult } from "./types";
import { AI_CRAWLERS } from "../constants";

export function generateRobotsTxt(baseUrl: string): string {
  const crawlerBlocks = AI_CRAWLERS.map(crawler => {
    const group = getCrawlerGroup(crawler);
    return `# ${group}\nUser-agent: ${crawler}\nAllow: /`;
  }).join("\n\n");

  return `# robots.txt â€” AI Search Optimized
# Generated by LLM Search (llmsearch.yourupdatedpage.xyz)

User-agent: *
Allow: /

${crawlerBlocks}

Sitemap: ${baseUrl}/sitemap.xml
`;
}

function getCrawlerGroup(crawler: string): string {
  if (["GPTBot", "OAI-SearchBot", "ChatGPT-User"].includes(crawler)) return "OpenAI";
  if (["ClaudeBot", "Claude-SearchBot"].includes(crawler)) return "Anthropic";
  if (["Google-Extended", "Gemini-Deep-Research"].includes(crawler)) return "Google AI";
  if (crawler === "PerplexityBot") return "Perplexity";
  if (crawler === "Applebot-Extended") return "Apple AI";
  if (crawler === "Amazonbot") return "Amazon";
  if (crawler === "Bingbot") return "Microsoft";
  if (crawler === "DuckAssistBot") return "DuckDuckGo AI";
  if (crawler === "YouBot") return "You.com";
  if (crawler === "meta-externalagent") return "Meta AI";
  if (crawler === "PhindBot") return "Phind";
  if (crawler === "cohere-ai") return "Cohere";
  if (crawler === "ExaBot") return "Exa";
  return "Other";
}

export function generateSitemapXml(pages: PageData[], baseUrl: string): string {
  const today = new Date().toISOString().split("T")[0];

  const entries = pages.map(page => {
    const isHome = page.path === "/" || page.path === "";
    const priority = isHome ? "1.0" : page.path.split("/").filter(Boolean).length <= 1 ? "0.8" : "0.6";
    const changefreq = isHome ? "weekly" : "monthly";

    return `  <url>
    <loc>${page.url}</loc>
    <lastmod>${today}</lastmod>
    <changefreq>${changefreq}</changefreq>
    <priority>${priority}</priority>
  </url>`;
  }).join("\n");

  return `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
${entries}
</urlset>
`;
}

export function generateLlmsTxt(pages: PageData[], baseUrl: string, siteName: string): string {
  // Group pages by type
  const home = pages.find(p => p.path === "/" || p.path === "");
  const regular = pages.filter(p => p.path !== "/" && p.path !== "");

  const homeDesc = home ? extractDescription(home) : siteName;

  let content = `# ${siteName}\n\n> ${homeDesc}\n\n## Pages\n`;

  if (home) {
    content += `- [Home](${home.url}): ${homeDesc}\n`;
  }

  for (const page of regular) {
    const desc = extractDescription(page);
    content += `- [${page.title || page.path}](${page.url}): ${desc}\n`;
  }

  return content;
}

export function generateLlmsFullTxt(pages: PageData[], baseUrl: string, siteName: string): string {
  const home = pages.find(p => p.path === "/" || p.path === "");
  const homeDesc = home ? extractDescription(home) : siteName;

  let content = `# ${siteName}\n\n> ${homeDesc}\n\n`;

  for (const page of pages) {
    const desc = extractDescription(page);
    const textContent = page.html
      .replace(/<script[\s\S]*?<\/script>/gi, "")
      .replace(/<style[\s\S]*?<\/style>/gi, "")
      .replace(/<[^>]+>/g, " ")
      .replace(/\s+/g, " ")
      .trim()
      .substring(0, 3000);

    content += `## ${page.title || page.path}\n\n`;
    content += `- [${page.title || page.path}](${page.url}): ${desc}\n\n`;
    content += `${textContent}\n\n---\n\n`;
  }

  return content;
}

function extractDescription(page: PageData): string {
  // Try meta description first
  const metaMatch = page.html.match(/<meta\s+name=["']description["']\s+content=["']([^"']+)["']/i)
    || page.html.match(/<meta\s+content=["']([^"']+)["']\s+name=["']description["']/i);

  if (metaMatch && metaMatch[1].trim().length > 10) {
    return metaMatch[1].trim();
  }

  // Fall back to first meaningful paragraph
  const pMatch = page.html.match(/<p[^>]*>([^<]{20,})<\/p>/i);
  if (pMatch) {
    return pMatch[1].trim().substring(0, 160);
  }

  return page.title || "No description available";
}
